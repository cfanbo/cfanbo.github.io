---
title: linux的vm相关参数介绍
author: admin
type: post
date: 2011-06-29T07:34:20+00:00
url: /archives/10149
IM_contentdowned:
 - 1
categories:
 - 服务器
tags:
 - Linux
 - vm

---
1. 保证linux有足够的物理内存，可以调整vm的如下参数

vm.min\_free\_kbytes=409600;//默认值是3797，保证物理内存有足够空闲空间，防止突发性换页
vm.vfs\_cache\_pressure=200；//默认是100，增大这个参数设置了虚拟内存回收directory和i-node缓冲的倾向，这个值越大。越易回收
vm.swappiness=40 //缺省60，减少这个参数会使系统尽快通过swapout不使用的进程资源来释放更多的物理内存

**一般在rac的配置环境中配置这三个参数，可以防止换页引起节点短暂无响应，导致节点重启**
2. 改善io系统的性能
overcommit_memory = 0
vm.overcommit_ratio = 10 //默认值是50，用于虚拟内存的物理内存的百分比
vm.dirty_ratio = 20 //默认值是40，为了保持稳定，持续的写入，把这个值调整的小一些，经验值是20


vm.dirty\_background\_ratio //缺省数值是500，也就是5秒,如果系统要求稳定持续的写，可以适当降低该值，把峰值的写操作平均多次，也避免宕机丢失更多的数据
vm.dirty\_expire\_centisecs //缺省是3000，也就是30秒,如果系统写操作压力很大，可以适当减小该值，但也不要太小；建议设置为 1500

===============================================

vm的相关参数在/proc/sys目录下:**相关命令**

> sysctl -p  //修改vm参数后，运行这个命令可以立即生效

 sysctl -a  //查看所有的vm参数**与磁盘IO子系统有关的**

/proc/sys/vm/dirty_ratio
这个参数控制文件系统的文件系统写缓冲区的大小，单位是百分比，表示系统内存的百分比，表示当写缓冲使用到系统内存多少的时候，开始向磁盘写出数据。增大之会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时，应该降低其数值，一般启动上缺省是 10。下面是增大的方法：

> echo ’40’ > /proc/sys/vm/dirty_ratio

/proc/sys/vm/dirty_background_ratio
这个参数控制文件系统的pdflush进程，在何时刷新磁盘。单位是百分比，表示系统内存的百分比，意思是当写缓冲使用到系统内存多少的时候，pdflush开始向磁盘写出数据。增大之会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时，应该降低其数值，一般启动上缺省是 5。下面是增大的方法：

> echo ’20’ > /proc/sys/vm/dirty\_background\_ratio

/proc/sys/vm/dirty_writeback_centisecs
这个参数控制内核的脏数据刷新进程pdflush的运行间隔。单位是 1/100 秒。缺省数值是500，也就是 5 秒。如果你的系统是持续地写入动作，那么实际上还是降低这个数值比较好，这样可以把尖峰的写操作削平成多次写操作。设置方法如下：

> echo “200” > /proc/sys/vm/dirty\_writeback\_centisecs

如果你的系统是短期地尖峰式的写操作，并且写入数据不大（几十M/次）且内存有比较多富裕，那么应该增大此数值：

> echo “1000” > /proc/sys/vm/dirty\_writeback\_centisecs

/proc/sys/vm/dirty_expire_centisecs
这个参数声明Linux内核写缓冲区里面的数据多“旧”了之后，pdflush进程就开始考虑写到磁盘中去。
单位是 1/100秒。缺省是 3000，也就是 30 秒的数据就算旧了，将会刷新磁盘。对于特别重载的写操作来说，这个值适当缩小也是好的，但也不能缩小太多，因为缩小太多也会导致IO提高太快。建议设置为 1500，也就是15秒算旧。

> echo “1500” > /proc/sys/vm/dirty\_expire\_centisecs

当然，如果你的系统内存比较大，并且写入模式是间歇式的，并且每次写入的数据不大（比如几十M），那么这个值还是大些的好。

**与网络IO子系统有关的**

 ****

/proc/sys/net/ipv4/tcp_retrans_collapse
这个参数控制TCP双方Window协商出现错误的时候的一些重传的行为。但是在老的2.6的核（<2.6.18）里头，这个重传会导致kernel oops，kernel panic，所以，如果出现有
tcp\_retrans\_*样子的kernel panic，可以把这个参数给设置成0：

> echo ‘0’ > /proc/sys/net/ipv4/tcp\_retrans\_collapse

提高Linux应对短连接的负载能力
在存在大量短连接的情况下，Linux的TCP栈一般都会生成大量的 TIME_WAIT 状态的socket。
你可以用下面的命令看到：

> netstat -ant| grep -i time_wait

有时候，这个数目是惊人的：

> netstat -ant|grep -i time_wait |wc -l

可能会超过三四万。这个时候，我们需要修改 linux kernel 的 tcp time wait的时间，缩短之，有个 sysctl 参数貌似可以使用，它是 /proc/sys/net/ipv4/tcp\_fin\_timeout，缺省值是 60，也就是60秒，很多网上的资料都说将这个数值设置低一些就可以减少netstat 里面的TIME_WAIT状态，但是这个说法是错误的。经过认真阅读Linux的内核源代码，我们发现这个数值其实是输出用的，修改之后并没有真正的读回内核中进行使用，而内核中真正管用的是一个宏定义，在$KERNEL/include/net/tcp.h里面，有下面的行：

> #define TCP\_TIMEWAIT\_LEN (60\*HZ) /\* how long to wait to destroy TIME-WAIT \* state, about 60 seconds     \*/

而这个宏是真正控制 TCP TIME\_WAIT 状态的超时时间的。如果我们希望减少 TIME\_WAIT 状态的数目（从而节省一点点内核操作时间），那么可以把这个数值设置低一些，根据我们的测试，设置为 10秒比较合适，也就是把上面的修改为：

> #define TCP\_TIMEWAIT\_LEN (10\*HZ) /\* how long to wait to destroy TIME-WAIT \* state, about 60 seconds     \*/

然后重新编译内核，重启系统即可发现短连接造成的TIME_WAIT状态大大减少：

> netstat -ant | grep -i time_wait |wc -l

一般情况都可以至少减少2/3。也能相应提高系统应对短连接的速度。
/proc/irq/{number}/smp_affinity
在多 CPU 的环境中，还有一个中断平衡的问题，比如，网卡中断会教给哪个 CPU 处理，这个参数控制哪些 CPU 可以绑定 IRQ 中断。其中的 {number} 是对应设备的中断编号，
可以用下面的命令找出：

> cat /proc/interrupt

比如，一般 eth0 的 IRQ 编号是 16，所以控制 eth0 中断绑定的 /proc 文件名是/proc/irq/16/smp_affinity。上面这个命令还可以看到某些中断对应的CPU处理的次数，
缺省的时候肯定是不平衡的。

设置其值的方法很简单，smp_affinity 自身是一个位掩码（bitmask），特定的位对应特定的 CPU，这样，01 就意味着只有第一个 CPU 可以处理对应的中断，而 0f（0x1111）
意味着四个 CPU 都会参与中断处理。

几乎所有外设都有这个参数设置，可以关注一下。

这个数值的推荐设置，其实在很大程度上，让专门的CPU处理专门的中断是效率最高的，比如，给磁盘IO一个CPU，给网卡一个CPU，这样是比较合理的。

也可以参考：