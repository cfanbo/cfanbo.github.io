---
title: 如何禁止搜索引擎蜘蛛爬行
author: admin
type: post
date: 2011-03-14T00:30:57+00:00
url: /archives/7978
IM_contentdowned:
 - 1
categories:
 - 前端设计

---
**方法一、robots Meta标签**

robots.txt是放在网站中，文件级的网络蜘蛛授权；而robots Meta标签是放在网页中，一般用于部分网页需要单独设置的情况下。两者的功能是一样的。
Meta robots标签必须放在和之间，格式：

content中的值决定允许抓取的类型，必须同时包含两个值：是否允许索引（index）和是否跟踪链接（follow，也可以理解为是否允许沿着网页中的超级链接继续抓取）。共有4个参数可选，组成4个组合：
index,follow：允许抓取本页，允许跟踪链接。
index,nofollow：允许抓取本页，但禁止跟踪链接。
noindex,follow：禁止抓取本页，但允许跟踪链接。
noindex,nofllow：禁止抓取本页，同时禁止跟踪本页中的链接。
以上1和4还有另一种写法：
index,follow可以写成all，如：

noindex,nofollow可以写成none，如：

**方法二、创建robots.txt文本**
对于robots.txt文本的相关概念或者是协议我就不提了，主要是直接告诉大家这个文件的写法。
文件应该同时包含2个域，“User-agent:”和“Disallow:”，每条指令独立一行。


(1)User-agent:
指定允许哪些蜘蛛抓取，如果给出参数，则只有指定的蜘蛛能够抓取；如值为通配符“*”号，代表允许所有蜘蛛抓取。如：
User-agent: Googlebot
只允许Google的蜘蛛抓取；
User-agent: *
允许所有蜘蛛抓取。
注意：User-agent必须出现在第一行（有意义的行，注释除外），首先声明用户代理。
(2)Disallow:
指定禁止蜘蛛抓取的目录或文件，如：
Disallow: /help.php
禁止抓取根目录下help.php文件；
Disallow: /admin/
禁止抓取根目录下的admin子目录中任何内容；
Disallow:
值为空时，表示不限制，蜘蛛可以抓取站内任何内容。
Disallow: /
禁止了蜘蛛抓取根目录下的所有内容。
如果需要指定多个目录或文件，可以用多个“Disallow: 文件或目录名”来指定，但必须每一项单独一行。
示例：
禁止百度蜘蛛：
User-agent: baiduspider
Disallow: /
禁止谷歌蜘蛛：
User-agent: Googlebot
Disallow: /
禁止所有蜘蛛：
User-agent: *
Disallow: /